# High Performance (Mutliprocess/Multithreading) for Python and Julia 

## No.:
## Title:
## https links: 

[Julia Lang part] 

1.
What scientists must know about hardware to write fast code
https://github.com/jakobnissen/hardware_introduction

https://viralinstruction.com/posts/hardware/



2.
Julia for Economists Bootcamp, 2022
https://github.com/cpfiffer/julia-bootcamp-2022


https://github.com/cpfiffer/julia-bootcamp-2022#session-2-parallelization

https://www.youtube.com/watch?v=trhsvOAH0YI
https://github.com/cpfiffer/julia-bootcamp-2022/blob/main/session-2/parallelization-lecture.ipynb


https://github.com/cpfiffer/julia-bootcamp-2022#session-4-high-performance-julia

https://youtu.be/i35LlZWZl1g
https://github.com/cpfiffer/julia-bootcamp-2022/blob/main/session-4/speed-lecture.ipynb



3.
Hands-On Design Patterns and Best Practices with Julia
https://github.com/PacktPublishing/Hands-on-Design-Patterns-and-Best-Practices-with-Julia



4.
A quick introduction to data parallelism in Julia
https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/



5.
Parallelization
https://enccs.github.io/Julia-for-HPC/parallelization/


GPU programming
https://enccs.github.io/Julia-for-HPC/GPU/



6.
[ANN] Folds.jl: threaded, distributed, and GPU-based high-level data-parallel interface for Julia
https://discourse.julialang.org/t/ann-folds-jl-threaded-distributed-and-gpu-based-high-level-data-parallel-interface-for-julia/54701/3


https://github.com/JuliaFolds/ParallelMagics.jl

https://github.com/JuliaFolds



7.
Announcing composable multi-threaded parallelism in Julia
https://julialang.org/blog/2019/07/multithreading/



8.
Using Julia
https://www.carc.usc.edu/user-information/user-guides/software-and-programming/julia

# Parallel programming with Julia
Package	                Purpose
Base.Threads	        For explicit multi-threading
Distributed	            For explicit multi-processing
MPI.jl	                For interfacing to MPI libraries
DistributedArrays.jl	For working with distributed arrays
Elemental.jl	        For distributed linear algebra
ClusterManagers.jl	    For launching jobs via cluster job schedulers (e.g., Slurm)
Dagger.jl	            For asynchronous evaluations and workflows
CUDA.jl	                For interfacing to Nvidia CUDA GPUs



9.
ML ⇌ Science Colaboratory's workshop Introduction to Machine Learning.
Supervised learning: One step at a time
https://github.com/mlcolab/IntroML.jl/blob/main/notebooks/supervised_learning.jl

https://mlcolab.github.io/IntroML.jl/dev/supervised_learning.html



10.
The Enzyme High-Performance Automatic Differentiator of LLVM
https://github.com/EnzymeAD/Enzyme
High-performance automatic differentiation of LLVM.


https://github.com/EnzymeAD/Enzyme.jl
Julia bindings for the Enzyme automatic differentiator


https://github.com/EnzymeAD/oxide-enzyme
Enzyme integration into Rust. Experimental, do not use.



11.
Julia v1.7 Release Notes
https://github.com/JuliaLang/julia/blob/master/HISTORY.md#julia-v17-release-notes


Multidimensional Array Literals
https://julialang.org/blog/2021/11/julia-1.7-highlights/#multidimensional_array_literals



https://github.com/JuliaLang/julia/issues/39285

https://github.com/JuliaLang/julia/issues/45461

https://docs.julialang.org/en/v1/base/arrays/#Base.hvncat

https://github.com/JuliaLang/julia/blob/7e54f9a069df2b382f765d5574787293c816fe26/base/abstractarray.jl#L2119


https://github.com/JuliaLang/julia/blob/master/HISTORY.md#language-changes-1
Multiple successive semicolons in an array expresion were previously ignored (e.g., [1 ;; 2] == [1 ; 2]). This syntax is now used to separate dimensions (see New language features).

v1 = [1, 2] # 2-element Vector{Int64}:
v2 = [3, 4] # 2-element Vector{Int64}:

[v1, v2] # 2-element Vector{Vector{Int64}}:
# [1, 2]
# [3, 4]

[v1; v2] # 4-element Vector{Int64}:
# 1
# 2
# 3
# 4

[v1;; v2] # 2×2 Matrix{Int64}:
# 1  3
# 2  4

[1,2,3 ;; 4,5,6] # syntax: unexpected semicolon in array expression

[1;2;3 ;;4;5;6] #3×2 Matrix{Int64}:
# 1  4
# 2  5
# 3  6


Meta.@lower [1 2;;; 3 4]
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = Core.tuple(1, 2, 2)
│   %2 = Base.hvncat(%1, true, 1, 2, 3, 4)
└──      return %2
))))

Meta.@lower [v1;;v2]
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = Base.hvncat(2, v1, v2)
└──      return %1
))))

Meta.@lower [v1;v2]
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = Base.vcat(v1, v2)
└──      return %1
))))

Meta.@lower [v1, v2]
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = Base.vect(v1, v2)
└──      return %1
))))

Meta.@lower[1 2;;
       3 4]
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = Base.hcat(1, 2, 3, 4)
└──      return %1
))))


Property Destructuring
https://julialang.org/blog/2021/11/julia-1.7-highlights/#property_destructuring

https://github.com/JuliaLang/julia/blob/master/HISTORY.md#new-language-features-1
(; a, b) = x can now be used to destructure properties a and b of x. This syntax is equivalent to a = getproperty(x, :a); b = getproperty(x, :b)





New features coming in Julia 1.7
https://lwn.net/Articles/871486/


https://julialang.org/blog/2021/11/julia-1.7-highlights/



12.
Concurrency in Julia
https://lwn.net/Articles/875367/

The Julia programming language has its roots in high-performance scientific computing, 
so it is no surprise that it has facilities for concurrent processing. 
Those features are not well-known outside of the Julia community, 
though, so it is interesting to see the different types of parallel and concurrent computation that the language supports. 
In addition, the upcoming release of Julia version 1.7 brings an improvement to the language's concurrent-computation palette, 
in the form of "task migration".



13.
libblastrampoline + MKL.jl
https://julialang.org/blog/2021/11/julia-1.7-highlights/#libblastrampoline_mkljl
Julia v1.7 introduces a new BLAS demuxing library called libblastrampoline (LBT), 
that provides a flexible and efficient way to switch the backing BLAS library at runtime. 
Because the BLAS/LAPACK API is "pure" (e.g. each BLAS/LAPACK invocation is separate from any other; 
there is no carryover state from one API call to another) it is possible to switch 
which BLAS backend actually services a particular client API call, such as a DGEMM call for
a Float64 Matrix-Matrix multiplication. This statelessness enables us to easily switch from one BLAS backend
to another without needing to modify client code, and combining this with a flexible wrapper implementation, 
we are able to provide a single, coherent API that automatically adjusts for a variety of BLAS/LAPACK providers 
across all the platforms that Julia itself supports.



14.
https://runebook.dev/en/docs/julia/-index-

https://runebook.dev



15.
Juliaで見る漸近理論 (確率変数でない数列の収束) + Real Analysis (Convergence & Bounded Series)
https://zenn.dev/hessihan/articles/26144a5ffb932a


INTRODUCTION TO THE CONVERGENCE OF SEQUENCES
https://math.uchicago.edu/~may/REU2015/REUPapers/Lytle.pdf


Convergent Sequences
https://users.math.msu.edu/users/zhan/Notes1.pdf

Math 320, Section 4: Analysis I
https://users.math.msu.edu/users/zhan/MTH320.html


Real Analysis Oral Exam study notes
http://www.math.toronto.edu/mnica/oral/real_notes.pdf



16.
Julia lang Garbage Collection

How much do collections of allocated objects cost?
https://bkamins.github.io/julialang/2021/06/11/vecvec.html


Julia gc.c 
https://github.com/JuliaLang/julia/blob/master/src/gc.c


On the garbage collection
https://discourse.julialang.org/t/on-the-garbage-collection/35695

https://discourse.julialang.org/t/on-the-garbage-collection/35695/8


Details about Julia’s Garbage Collector, Reference Counting?
https://discourse.julialang.org/t/details-about-julias-garbage-collector-reference-counting/18021/3


17.
Julia Learning Circle: JIT and Method Invalidations 
https://wesselb.github.io/2020/11/07/julia-learning-circle-meeting-1.html



18.
Solutionf For Hihg-Dimensional Statistics 
https://wesselb.github.io/2020/08/21/high-dimensional-statistics.html


High-Dimensional Statistics A Non-Asymptotic Viewpoint - Martin J. Wainwright, University of California, Berkeley
https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E

DOI:https://doi.org/10.1017/9781108627771

https://high-dimensional-statistics.github.io

https://www.cambridge.org/core/services/aop-cambridge-core/content/view/30AF7B572184787F4C99715838549721/9781108498029c2_21-57.pdf/basic_tail_and_concentration_bounds.pdf



18-1.
High-Dimensional Probability - An Introduction with Applications in Data Science
Roman Vershynin
https://www.math.uci.edu/~rvershyn/

https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.html#

https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf



19.
Julia Learning Circle: Memoty Allocations And Garbage Collection 
https://wesselb.github.io/2020/11/23/julia-learning-circle-meeting-2.html


Julia Learning Circle: Generated Functins 
https://wesselb.github.io/2020/12/13/julia-learning-circle-meeting-3.html



20.
JuliaNotes.jl
https://m3g.github.io/JuliaNotes.jl/stable/memory/


Vector{Int} <: Vector{Real} is false??
https://m3g.github.io/JuliaNotes.jl/stable/typevariance/


Assignment and mutation
https://m3g.github.io/JuliaNotes.jl/stable/assignment/


Workflows for developing effectivelly in Julia
https://m3g.github.io/JuliaNotes.jl/stable/workflow/



21.
Julia v1.8 Release Notes
https://github.com/JuliaLang/julia/blob/master/HISTORY.md#julia-v18-release-notes

Compiler/Runtime improvements

21-1. libjulia-codegen
The LLVM-based compiler has been separated from the run-time library into a new library, libjulia-codegen. It is loaded by default, so normal usage should see no changes. In deployments that do not need the compiler (e.g. system images where all needed code is precompiled), this library (and its LLVM dependency) can simply be excluded (#41936).
https://github.com/JuliaLang/julia/issues/41936

Unreasonably large executable size from create_app #660
https://github.com/JuliaLang/PackageCompiler.jl/issues/660

This is expected. Julia currently has no good way to run code without these supporting libraries. But there is work in progress of trying to improve this.


21-2. Base.@assume_effects macro
Inference now tracks various effects such as side-effectful-ness and nothrow-ness on a per-specialization basis. Code heavily dependent on constant propagation should see significant compile-time performance improvements and certain cases (e.g. calls to uninlinable functions that are nevertheless effect free) should see runtime performance improvements. Effects may be overwritten manually with the Base.@assume_effects macro (#43852).
https://github.com/JuliaLang/julia/issues/43852

https://github.com/JuliaLang/julia/commit/ef4220533d4a9a887b199362e37de0e056c1a458

improve concrete-foldability of core math functions #45613
https://github.com/JuliaLang/julia/pull/45613

Matrix{Int} \ Vector{Float32} is type-unstable #45696
https://github.com/JuliaLang/julia/issues/45696

Linear system solve promotes Float32 to Float64 #1041
https://github.com/JuliaArrays/StaticArrays.jl/issues/1041


21-3.
Bootstrapping time has been improved by about 25% (#41794).
https://github.com/JuliaLang/julia/issues/41794



22.
ENGR108: Introduction to Matrix Methods (Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares)
https://stanford.edu/class/engr108/

I just want to know how to get inverse matrix as using Julialang 
https://stanford.edu/class/engr108/lectures/julia_inverses_slides.pdf

https://stanford.edu/class/engr108/lectures/

https://stanford.edu/class/engr108/lectures/julia_least_squares_slides.pdf

https://stanford.edu/class/engr108/lectures/julia_vectors_slides.pdf

https://stanford.edu/class/engr108/lectures/julia_matrices_slides.pdf



23.
Language introspection
https://juliateachingctu.github.io/Scientific-Programming-in-Julia/dev/lecture_06/lecture/

JuliaTeachingCTU/ Scientific-Programming-in-Julia
https://github.com/JuliaTeachingCTU/Scientific-Programming-in-Julia/blob/master/docs/src/index.md#

This repository contains all the course materials for the master course Scientific Programming in Julia 
taught at the Czech Techincal University in Prague. 
You can find more information on the official course website.

https://juliateachingctu.github.io/Scientific-Programming-in-Julia/stable/



Stages of compilation
Julia (as any modern compiler) uses several stages to convert source code to native code. Let's recap them

parsing the source code to abstract syntax tree (AST)
lowering the abstract syntax tree static single assignment form (SSA) see wiki
assigning types to variables and performing type inference on called functions
lowering the typed code to LLVM intermediate representation (LLVM Ir)
using LLVM compiler to produce a native code.

function nextfib(n)
    a, b = one(n), one(n)
    while b < n 
        a, b = b, a + b 
    end
    return b 
end

Meta.parse(
    """ function nextfib(n)
            a, b = nextfib(n)
            while b < n 
                a, b = b, a + b 
            end
            return b 
        end   
    """)

# For inserted debugging information, there is an option to pass keyword argument debuginfo=:source.
@code_lowered debuginfo=:source nextfib(3)
@code_lowered nextfib(3)


@code_typed nextfib(3)
@code_warntype nextfib(3)


@code_llvm debuginfo=:source nextfib(3)
@code_llvm nextfib(3)

@code_native debuginfo=:source nextfib(3)
@code_native nextfib(3)

@time 
@allocated 
@which 

using BenchmarkTools 
@btime 



24.
JuliaTeachingCTU/ Julia-for-Optimization-and-Learning
https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning

https://github.com/JuliaTeachingCTU/Julia-for-Optimization-and-Learning/tree/master/docs/src

https://juliateachingctu.github.io/Julia-for-Optimization-and-Learning/stable/

What will we emphasize?
The main goals of the course are the following:

You will learn the connections between theory and coding. There are many lectures which teach either only theory or only coding. We will show you both.
You will learn how to code efficiently. We will teach you to split the code into small parts which are simpler to debug or optimize. We will often show you several writing possibilities and comment on the differences.
You will learn about machine learning and neural networks. You will understand neural networks by writing a simple one from scratch. Then you will learn how to use packages to write simple code for complicated networks.
You will learn independence. The problem formulation of many exercises is very general, which simulates when no step-by-step procedure is provided.


Tyeps system and generic programming 
https://juliateachingctu.github.io/Julia-for-Optimization-and-Learning/stable/lecture_06/compositetypes/


Optimization 
https://juliateachingctu.github.io/Julia-for-Optimization-and-Learning/stable/lecture_08/theory/



25.
Julia Parallel, Multithreading, Multiprocess 

A quick introduction to data parallelism in Julia
https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/


Parallelization
https://enccs.github.io/Julia-for-HPC/parallelization/





https://github.com/JuliaParallel/Dagger.jl
A framework for out-of-core and parallel computing

At the core of Dagger.jl is a scheduler heavily inspired by Dask. 
It can run computations represented as directed-acyclic-graphs (DAGs) efficiently on many Julia worker processes and threads, 
as well as GPUs via DaggerGPU.jl.


https://github.com/JuliaParallel/DTables.jl

DTable – an early performance assessment of a new distributed table implementation
https://julialang.org/blog/2021/12/dtable-performance-assessment/


https://juliaparallel.org/Dagger.jl/stable/dtable/
The DTable, or "distributed table", is an abstraction layer on top of Dagger that allows loading table-like structures into a distributed environment. 
The main idea is that a Tables.jl-compatible source provided by the user gets partitioned into several parts and stored as Chunks. 
These can then be distributed across worker processes by the scheduler as operations are performed on the containing DTable.


https://github.com/JuliaParallel/DistributedArrays.jl
Distributed arrays for Julia.

DistributedArrays.jl uses the stdlib Distributed to implement a Global Array interface. 
A DArray is distributed across a set of workers. 
Each worker can read and write from its local portion of the array and each worker has read-only access to 
the portions of the array held by other workers.


https://github.com/JuliaParallel/MPI.jl
This provides Julia interface to the Message Passing Interface (MPI), roughly inspired by mpi4py.

"Julia at Scale" topic on the Julia Discourse
https://discourse.julialang.org/c/domain/parallel/34


https://github.com/JuliaParallel/Elemental.jl
A package for dense and sparse distributed linear algebra and optimization. 
The underlying functionality is provided by the C++ library Elemental written originally by Jack Poulson and now maintained by LLNL.


A Julia interface to Apache Spark™
http://dfdx.github.io/Spark.jl/dev/

https://github.com/dfdx/Spark.jl

https://spark.apache.org/downloads.html


https://github.com/JuliaFolds/Folds.jl
Folds.jl provides a unified interface for sequential, threaded, and distributed folds.

[ANN] Folds.jl: threaded, distributed, and GPU-based high-level data-parallel interface for Julia
https://discourse.julialang.org/t/ann-folds-jl-threaded-distributed-and-gpu-based-high-level-data-parallel-interface-for-julia/54701


Julia for Economists - Parallelization for Fun and Profit - Cameron Pfiffer (cpfiffer@stanford.edu)
https://github.com/cpfiffer/julia-bootcamp-2022/blob/main/session-2/parallelization-lecture.ipynb


https://github.com/JuliaFolds/FLoops.jl
FLoops.jl provides a macro @floop. It can be used to generate a fast generic sequential and parallel iteration over complex collections.


https://github.com/JuliaFolds/ParallelMagics.jl
ParallelMagics.jl is aiming at providing safe parallelism to Julia programmers such that
"No-brainer" parallelism using compiler analysis; i.e., the code is parallelized only if the compiler guarantees the safety.


https://github.com/JuliaFolds/FoldsCUDA.jl
FoldsCUDA.jl provides Transducers.jl-compatible fold (reduce) implemented using CUDA.jl. 
This brings the transducers and reducing function combinators implemented in Transducers.jl to GPU. 
Furthermore, using FLoops.jl, you can write parallel for loops that run on GPU.


https://github.com/JuliaFolds/Transducers.jl
Transducers.jl provides composable algorithms on "sequence" of inputs. They are called transducers, first introduced in Clojure language by Rich Hickey.

A quick introduction to data parallelism in Julia
https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/


https://juliafolds.github.io/Transducers.jl/dev/



/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/

[Python part]

1.
多執行緒 — Python Threading
https://medium.com/ching-i/%E5%A4%9A%E5%9F%B7%E8%A1%8C%E7%B7%92-python-threading-52e1dfb3d5c9

https://github.com/chingi071/Thread

多執行緒
https://medium.com/ching-i/%E5%A4%9A%E5%9F%B7%E8%A1%8C%E7%B7%92-de16f92944c8



2.
Concurrency in Python - Quick Guide
https://www.tutorialspoint.com/concurrency_in_python/concurrency_in_python_quick_guide.htm



3.
Python Concurrency Tutorial
https://medium.com/@santiagobasulto/python-concurrency-tutorial-a5a8aee3b595

https://github.com/santiagobasulto/pycon-concurrency-tutorial-2020



4.
Python 的 concurrency 和 parallelization
https://medium.com/@alan81920/python-%E7%9A%84-concurrency-%E5%92%8C-parallelization-efeddcb30c4c



5.
Asynchronous Code
https://fastapi.tiangolo.com/async/#technical-details



6.
asyncio — Asynchronous I/O
https://docs.python.org/3.9/library/asyncio.html


https://bbc.github.io/cloudfit-public-docs/asyncio/asyncio-part-1.html


https://djangostars.com/blog/asynchronous-programming-in-python-asyncio/


https://www.datacamp.com/tutorial/asyncio-introduction



