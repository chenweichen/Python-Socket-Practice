# High Performance (Mutliprocess/Multithreading) for Python and Julia 

## No.:
## Title:
## https links: 

[Julia Lang part] 

1.
What scientists must know about hardware to write fast code
https://github.com/jakobnissen/hardware_introduction

https://viralinstruction.com/posts/hardware/



2.
Julia for Economists Bootcamp, 2022
https://github.com/cpfiffer/julia-bootcamp-2022


https://github.com/cpfiffer/julia-bootcamp-2022#session-2-parallelization

https://www.youtube.com/watch?v=trhsvOAH0YI
https://github.com/cpfiffer/julia-bootcamp-2022/blob/main/session-2/parallelization-lecture.ipynb


https://github.com/cpfiffer/julia-bootcamp-2022#session-4-high-performance-julia

https://youtu.be/i35LlZWZl1g
https://github.com/cpfiffer/julia-bootcamp-2022/blob/main/session-4/speed-lecture.ipynb



3.
Hands-On Design Patterns and Best Practices with Julia
https://github.com/PacktPublishing/Hands-on-Design-Patterns-and-Best-Practices-with-Julia



4.
A quick introduction to data parallelism in Julia
https://juliafolds.github.io/data-parallelism/tutorials/quick-introduction/



5.
Parallelization
https://enccs.github.io/Julia-for-HPC/parallelization/


GPU programming
https://enccs.github.io/Julia-for-HPC/GPU/



6.
[ANN] Folds.jl: threaded, distributed, and GPU-based high-level data-parallel interface for Julia
https://discourse.julialang.org/t/ann-folds-jl-threaded-distributed-and-gpu-based-high-level-data-parallel-interface-for-julia/54701/3


https://github.com/JuliaFolds/ParallelMagics.jl

https://github.com/JuliaFolds



7.
Announcing composable multi-threaded parallelism in Julia
https://julialang.org/blog/2019/07/multithreading/



8.
Using Julia
https://www.carc.usc.edu/user-information/user-guides/software-and-programming/julia

# Parallel programming with Julia
Package	                Purpose
Base.Threads	        For explicit multi-threading
Distributed	            For explicit multi-processing
MPI.jl	                For interfacing to MPI libraries
DistributedArrays.jl	For working with distributed arrays
Elemental.jl	        For distributed linear algebra
ClusterManagers.jl	    For launching jobs via cluster job schedulers (e.g., Slurm)
Dagger.jl	            For asynchronous evaluations and workflows
CUDA.jl	                For interfacing to Nvidia CUDA GPUs



9.
ML ⇌ Science Colaboratory's workshop Introduction to Machine Learning.
Supervised learning: One step at a time
https://github.com/mlcolab/IntroML.jl/blob/main/notebooks/supervised_learning.jl

https://mlcolab.github.io/IntroML.jl/dev/supervised_learning.html



10.
The Enzyme High-Performance Automatic Differentiator of LLVM
https://github.com/EnzymeAD/Enzyme
High-performance automatic differentiation of LLVM.


https://github.com/EnzymeAD/Enzyme.jl
Julia bindings for the Enzyme automatic differentiator


https://github.com/EnzymeAD/oxide-enzyme
Enzyme integration into Rust. Experimental, do not use.



11.
Julia v1.7 Release Notes
https://github.com/JuliaLang/julia/blob/master/HISTORY.md#julia-v17-release-notes


Multidimensional Array Literals
https://julialang.org/blog/2021/11/julia-1.7-highlights/#multidimensional_array_literals



https://github.com/JuliaLang/julia/issues/39285

https://github.com/JuliaLang/julia/issues/45461

https://docs.julialang.org/en/v1/base/arrays/#Base.hvncat

https://github.com/JuliaLang/julia/blob/7e54f9a069df2b382f765d5574787293c816fe26/base/abstractarray.jl#L2119


https://github.com/JuliaLang/julia/blob/master/HISTORY.md#language-changes-1
Multiple successive semicolons in an array expresion were previously ignored (e.g., [1 ;; 2] == [1 ; 2]). This syntax is now used to separate dimensions (see New language features).

v1 = [1, 2] # 2-element Vector{Int64}:
v2 = [3, 4] # 2-element Vector{Int64}:

[v1, v2] # 2-element Vector{Vector{Int64}}:
# [1, 2]
# [3, 4]

[v1; v2] # 4-element Vector{Int64}:
# 1
# 2
# 3
# 4

[v1;; v2] # 2×2 Matrix{Int64}:
# 1  3
# 2  4

[1,2,3 ;; 4,5,6] # syntax: unexpected semicolon in array expression

[1;2;3 ;;4;5;6] #3×2 Matrix{Int64}:
# 1  4
# 2  5
# 3  6


Meta.@lower [1 2;;; 3 4]
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = Core.tuple(1, 2, 2)
│   %2 = Base.hvncat(%1, true, 1, 2, 3, 4)
└──      return %2
))))

Meta.@lower [v1;;v2]
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = Base.hvncat(2, v1, v2)
└──      return %1
))))

Meta.@lower [v1;v2]
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = Base.vcat(v1, v2)
└──      return %1
))))

Meta.@lower [v1, v2]
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = Base.vect(v1, v2)
└──      return %1
))))

Meta.@lower[1 2;;
       3 4]
:($(Expr(:thunk, CodeInfo(
    @ none within `top-level scope`
1 ─ %1 = Base.hcat(1, 2, 3, 4)
└──      return %1
))))


Property Destructuring
https://julialang.org/blog/2021/11/julia-1.7-highlights/#property_destructuring

https://github.com/JuliaLang/julia/blob/master/HISTORY.md#new-language-features-1
(; a, b) = x can now be used to destructure properties a and b of x. This syntax is equivalent to a = getproperty(x, :a); b = getproperty(x, :b)





New features coming in Julia 1.7
https://lwn.net/Articles/871486/


https://julialang.org/blog/2021/11/julia-1.7-highlights/



12.
Concurrency in Julia
https://lwn.net/Articles/875367/

The Julia programming language has its roots in high-performance scientific computing, 
so it is no surprise that it has facilities for concurrent processing. 
Those features are not well-known outside of the Julia community, 
though, so it is interesting to see the different types of parallel and concurrent computation that the language supports. 
In addition, the upcoming release of Julia version 1.7 brings an improvement to the language's concurrent-computation palette, 
in the form of "task migration".



13.
libblastrampoline + MKL.jl
https://julialang.org/blog/2021/11/julia-1.7-highlights/#libblastrampoline_mkljl
Julia v1.7 introduces a new BLAS demuxing library called libblastrampoline (LBT), 
that provides a flexible and efficient way to switch the backing BLAS library at runtime. 
Because the BLAS/LAPACK API is "pure" (e.g. each BLAS/LAPACK invocation is separate from any other; 
there is no carryover state from one API call to another) it is possible to switch 
which BLAS backend actually services a particular client API call, such as a DGEMM call for
a Float64 Matrix-Matrix multiplication. This statelessness enables us to easily switch from one BLAS backend
to another without needing to modify client code, and combining this with a flexible wrapper implementation, 
we are able to provide a single, coherent API that automatically adjusts for a variety of BLAS/LAPACK providers 
across all the platforms that Julia itself supports.



14.
https://runebook.dev/en/docs/julia/-index-

https://runebook.dev



/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/*#-/

[Python part]

1.
多執行緒 — Python Threading
https://medium.com/ching-i/%E5%A4%9A%E5%9F%B7%E8%A1%8C%E7%B7%92-python-threading-52e1dfb3d5c9

https://github.com/chingi071/Thread

多執行緒
https://medium.com/ching-i/%E5%A4%9A%E5%9F%B7%E8%A1%8C%E7%B7%92-de16f92944c8



2.
Concurrency in Python - Quick Guide
https://www.tutorialspoint.com/concurrency_in_python/concurrency_in_python_quick_guide.htm



3.
Python Concurrency Tutorial
https://medium.com/@santiagobasulto/python-concurrency-tutorial-a5a8aee3b595

https://github.com/santiagobasulto/pycon-concurrency-tutorial-2020



4.
Python 的 concurrency 和 parallelization
https://medium.com/@alan81920/python-%E7%9A%84-concurrency-%E5%92%8C-parallelization-efeddcb30c4c



5.
Asynchronous Code
https://fastapi.tiangolo.com/async/#technical-details



6.
asyncio — Asynchronous I/O
https://docs.python.org/3.9/library/asyncio.html


https://bbc.github.io/cloudfit-public-docs/asyncio/asyncio-part-1.html


https://djangostars.com/blog/asynchronous-programming-in-python-asyncio/


https://www.datacamp.com/tutorial/asyncio-introduction



